{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# RQ-VAE Model\n",
    "class RQVAE(nn.Module):\n",
    "    def __init__(self, input_dim=768, latent_dim=32, codebook_size=256, num_codebooks=3):\n",
    "        super(RQVAE, self).__init__()\n",
    "        \n",
    "        # Encoder: Compress the input embedding (768 dimensions) to a latent representation\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)  # Output latent representation (32 dimensions)\n",
    "        )\n",
    "        \n",
    "        # Residual Quantizer: Multiple codebooks for residual quantization\n",
    "        self.codebooks = nn.ModuleList([\n",
    "            nn.Embedding(codebook_size, latent_dim) for _ in range(num_codebooks)\n",
    "        ])\n",
    "        self.codebook_size = codebook_size\n",
    "        self.num_codebooks = num_codebooks\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Decoder: Decode the quantized representation (32 dimensions) back to input embedding\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim)  # Reconstruct the original input embedding (768 dimensions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode input to latent representation\n",
    "        latent = self.encoder(x)\n",
    "        \n",
    "        # Residual quantization\n",
    "        quantized = torch.zeros_like(latent)  # Initialize quantized representation\n",
    "        residual = latent\n",
    "        codes = []  # Store semantic tokens (indices from codebooks)\n",
    "        \n",
    "        for i, codebook in enumerate(self.codebooks):\n",
    "            # Compute distances between residual and codebook vectors\n",
    "            distances = torch.cdist(residual.unsqueeze(1), codebook.weight.unsqueeze(0))  # (batch_size, 1, codebook_size)\n",
    "            indices = torch.argmin(distances, dim=-1).squeeze(1)  # Get closest codebook vector indices\n",
    "            codes.append(indices)  # Save the selected codebook indices\n",
    "            \n",
    "            # Add selected codebook vector to quantized representation\n",
    "            quantized += codebook(indices)\n",
    "            \n",
    "            # Update residual\n",
    "            residual = residual - codebook(indices)\n",
    "        \n",
    "        # Decode the quantized representation\n",
    "        reconstructed = self.decoder(quantized)\n",
    "        \n",
    "        return reconstructed, codes\n",
    "\n",
    "# Loss function for RQ-VAE\n",
    "class RQVAE_Loss(nn.Module):\n",
    "    def __init__(self, beta=0.25):\n",
    "        super(RQVAE_Loss, self).__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, input, reconstructed, quantized, latent):\n",
    "        # Reconstruction loss (MSE)\n",
    "        recon_loss = F.mse_loss(reconstructed, input)\n",
    "        \n",
    "        # Quantization loss (distance between latent and quantized)\n",
    "        quant_loss = F.mse_loss(latent, quantized)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = recon_loss + self.beta * quant_loss\n",
    "        return total_loss\n",
    "\n",
    "# Training example\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_dim = 768\n",
    "    latent_dim = 32\n",
    "    codebook_size = 256\n",
    "    num_codebooks = 3\n",
    "    batch_size = 1024\n",
    "    learning_rate = 0.4\n",
    "    epochs = 20000\n",
    "    \n",
    "    # Create RQ-VAE model\n",
    "    model = RQVAE(input_dim=input_dim, latent_dim=latent_dim, codebook_size=codebook_size, num_codebooks=num_codebooks)\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    criterion = RQVAE_Loss(beta=0.25)\n",
    "    \n",
    "    # Dummy dataset (replace with actual embeddings from Sentence-T5)\n",
    "    input_embeddings = torch.randn(batch_size, input_dim)  # Random input embeddings\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed, codes = model(input_embeddings)\n",
    "        \n",
    "        # Compute loss\n",
    "        latent = model.encoder(input_embeddings)\n",
    "        quantized = torch.zeros_like(latent)\n",
    "        residual = latent\n",
    "        for i, codebook in enumerate(model.codebooks):\n",
    "            indices = codes[i]\n",
    "            quantized += codebook(indices)\n",
    "            residual = residual - codebook(indices)\n",
    "        loss = criterion(input_embeddings, reconstructed, quantized, latent)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss every 1000 epochs\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# RQ-VAE Model\n",
    "class RQVAE(nn.Module):\n",
    "    def __init__(self, input_dim=768, latent_dim=32, codebook_size=256, num_codebooks=3):\n",
    "        super(RQVAE, self).__init__()\n",
    "        \n",
    "        # Encoder: Compress the input embedding to a latent representation\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)  # Output latent representation (32 dimensions)\n",
    "        )\n",
    "        \n",
    "        # Residual Quantizer: Placeholder for codebooks (will be initialized with k-means)\n",
    "        self.codebooks = nn.ModuleList([\n",
    "            nn.Embedding(codebook_size, latent_dim) for _ in range(num_codebooks)\n",
    "        ])\n",
    "        self.codebook_size = codebook_size\n",
    "        self.num_codebooks = num_codebooks\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Decoder: Decode the quantized representation back to input embedding\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim)  # Reconstruct the original input embedding\n",
    "        )\n",
    "\n",
    "    def initialize_codebooks(self, data):\n",
    "        \"\"\"\n",
    "        Use k-means clustering to initialize the codebook vectors.\n",
    "        Args:\n",
    "            data (torch.Tensor): Input data used for k-means initialization (latent representations).\n",
    "        \"\"\"\n",
    "        latent = self.encoder(data).detach().cpu().numpy()  # Encode data to latent space\n",
    "        \n",
    "        residual = latent  # Initialize residual as the latent representation\n",
    "        for i, codebook in enumerate(self.codebooks):\n",
    "            # Perform k-means clustering on the residual\n",
    "            kmeans = KMeans(n_clusters=self.codebook_size, random_state=42)\n",
    "            kmeans.fit(residual)\n",
    "            # Set the codebook weights to the cluster centers\n",
    "            codebook.weight.data.copy_(torch.tensor(kmeans.cluster_centers_, dtype=torch.float32))\n",
    "            # Update residual by subtracting the nearest cluster center\n",
    "            residual = residual - kmeans.cluster_centers_[kmeans.labels_]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode input to latent representation\n",
    "        latent = self.encoder(x)\n",
    "        \n",
    "        # Residual quantization\n",
    "        quantized = torch.zeros_like(latent)  # Initialize quantized representation\n",
    "        residual = latent\n",
    "        codes = []  # Store semantic tokens (indices from codebooks)\n",
    "        \n",
    "        for i, codebook in enumerate(self.codebooks):\n",
    "            # Compute distances between residual and codebook vectors\n",
    "            distances = torch.cdist(residual.unsqueeze(1), codebook.weight.unsqueeze(0))  # (batch_size, 1, codebook_size)\n",
    "            indices = torch.argmin(distances, dim=-1).squeeze(1)  # Get closest codebook vector indices\n",
    "            codes.append(indices)  # Save the selected codebook indices\n",
    "            \n",
    "            # Add selected codebook vector to quantized representation\n",
    "            quantized += codebook(indices)\n",
    "            \n",
    "            # Update residual\n",
    "            residual = residual - codebook(indices)\n",
    "        \n",
    "        # Decode the quantized representation\n",
    "        reconstructed = self.decoder(quantized)\n",
    "        \n",
    "        return reconstructed, codes\n",
    "\n",
    "# Loss function for RQ-VAE\n",
    "class RQVAE_Loss(nn.Module):\n",
    "    def __init__(self, beta=0.25):\n",
    "        super(RQVAE_Loss, self).__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, input, reconstructed, quantized, latent):\n",
    "        # Reconstruction loss (MSE)\n",
    "        recon_loss = F.mse_loss(reconstructed, input)\n",
    "        \n",
    "        # Quantization loss (distance between latent and quantized)\n",
    "        quant_loss = F.mse_loss(latent, quantized)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = recon_loss + self.beta * quant_loss\n",
    "        return total_loss\n",
    "\n",
    "# Training example\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_dim = 768\n",
    "    latent_dim = 32\n",
    "    codebook_size = 256\n",
    "    num_codebooks = 3\n",
    "    batch_size = 1024\n",
    "    learning_rate = 0.4\n",
    "    epochs = 20000\n",
    "    \n",
    "    # Create RQ-VAE model\n",
    "    model = RQVAE(input_dim=input_dim, latent_dim=latent_dim, codebook_size=codebook_size, num_codebooks=num_codebooks)\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    criterion = RQVAE_Loss(beta=0.25)\n",
    "    \n",
    "    # Dummy dataset (replace with actual embeddings from Sentence-T5)\n",
    "    input_embeddings = torch.randn(batch_size, input_dim)  # Random input embeddings\n",
    "\n",
    "    # Initialize codebooks with k-means\n",
    "    model.initialize_codebooks(input_embeddings)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed, codes = model(input_embeddings)\n",
    "        \n",
    "        # Compute loss\n",
    "        latent = model.encoder(input_embeddings)\n",
    "        quantized = torch.zeros_like(latent)\n",
    "        residual = latent\n",
    "        for i, codebook in enumerate(model.codebooks):\n",
    "            indices = codes[i]\n",
    "            quantized += codebook(indices)\n",
    "            residual = residual - codebook(indices)\n",
    "        loss = criterion(input_embeddings, reconstructed, quantized, latent)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss every 1000 epochs\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Codebook(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(Codebook, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_embeddings, embedding_dim))  # 初始化 codebook 权重\n",
    "\n",
    "    def forward(self, residual):\n",
    "        # 计算 residual 和 codebook 的距离\n",
    "        distances = torch.cdist(residual.unsqueeze(1), self.weight.unsqueeze(0))  # (batch_size, 1, num_embeddings)\n",
    "        indices = torch.argmin(distances, dim=-1).squeeze(1)  # 最近邻索引 (batch_size,)\n",
    "        selected_vectors = self.weight[indices]  # 最近邻向量 (batch_size, embedding_dim)\n",
    "        return selected_vectors, indices\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. Transformer Encoder-Decoder Model\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, user_vocab_size, embed_dim=128, num_heads=6, num_layers=4, mlp_dim=1024, dropout=0.1):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        \n",
    "        # Embedding layers for Semantic ID tokens and User ID tokens\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)  # For semantic tokens\n",
    "        self.user_embedding = nn.Embedding(user_vocab_size, embed_dim)  # For user ID tokens\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(500, embed_dim))  # Max length = 500\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,          # 128\n",
    "            nhead=num_heads,            # 6\n",
    "            dim_feedforward=mlp_dim,    # 1024\n",
    "            dropout=dropout,            # 0.1\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers) # 4\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,          # 128\n",
    "            nhead=num_heads,            # 6\n",
    "            dim_feedforward=mlp_dim,    # 1024\n",
    "            dropout=dropout,            # 0.1\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers) # 4\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, user_ids, input_ids, target_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_ids (torch.Tensor): User ID tokens, shape (batch_size, 1)\n",
    "            input_ids (torch.Tensor): Input token IDs, shape (batch_size, seq_len)\n",
    "            target_ids (torch.Tensor): Target token IDs, shape (batch_size, target_len)\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Predicted logits, shape (batch_size, target_len, vocab_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        target_len = target_ids.shape[1]\n",
    "        \n",
    "        # User ID embedding\n",
    "        user_embedded = self.user_embedding(user_ids)  # Shape: (batch_size, 1, embed_dim)\n",
    "        \n",
    "        # Semantic ID embedding with positional encoding\n",
    "        input_embedded = self.token_embedding(input_ids) + self.positional_encoding[:seq_len, :]  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Concatenate user embedding and input embedding\n",
    "        input_embedded = torch.cat([user_embedded, input_embedded], dim=1)  # Shape: (batch_size, seq_len+1, embed_dim)\n",
    "        \n",
    "        # Encode input\n",
    "        memory = self.encoder(input_embedded.permute(1, 0, 2))  # Shape: (seq_len+1, batch_size, embed_dim)\n",
    "        \n",
    "        # Target embedding with positional encoding\n",
    "        target_embedded = self.token_embedding(target_ids) + self.positional_encoding[:target_len, :]  # Shape: (batch_size, target_len, embed_dim)\n",
    "        \n",
    "        # Decode\n",
    "        output = self.decoder(\n",
    "            tgt=target_embedded.permute(1, 0, 2),  # Shape: (target_len, batch_size, embed_dim)\n",
    "            memory=memory  # Shape: (seq_len+1, batch_size, embed_dim)\n",
    "        )  # Shape: (target_len, batch_size, embed_dim)\n",
    "        \n",
    "        # Project to vocab size\n",
    "        logits = self.output_projection(output.permute(1, 0, 2))  # Shape: (batch_size, target_len, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# 2. Training Configuration\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    vocab_size = 1024  # Total tokens (Semantic ID tokens)\n",
    "    user_vocab_size = 2000  # User ID tokens\n",
    "    embed_dim = 128\n",
    "    num_heads = 6\n",
    "    num_layers = 4\n",
    "    mlp_dim = 1024\n",
    "    dropout = 0.1\n",
    "    batch_size = 256\n",
    "    seq_len = 50  # Max sequence length\n",
    "    target_len = 3  # Semantic ID length (m=3)\n",
    "    learning_rate = 0.01\n",
    "    num_steps = 200000\n",
    "    \n",
    "    # Model\n",
    "    model = TransformerSeq2Seq(vocab_size, user_vocab_size, embed_dim, num_heads, num_layers, mlp_dim, dropout)\n",
    "    model.train()\n",
    "    \n",
    "    # Optimizer and Loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lr_lambda=lambda step: min((step + 1) ** -0.5, (step + 1) * (10000 ** -1.5))\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Dummy data (replace with real data)\n",
    "    user_ids = torch.randint(0, user_vocab_size, (batch_size, 1))  # Random user IDs\n",
    "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # Random input sequences\n",
    "    target_ids = torch.randint(0, vocab_size, (batch_size, target_len))  # Random target sequences\n",
    "    \n",
    "    # Training loop\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(user_ids, input_ids, target_ids)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log progress\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            print(f\"Step [{step + 1}/{num_steps}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
